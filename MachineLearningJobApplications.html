<!DOCTYPE html>
<html>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Custom Machine Learning for better Job Applications</title>
        <link rel="stylesheet" href="style.css">
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-csharp.min.js"></script>
    </head>
    <body class="article">
        <div class="header">
            <a class="back" href="index.html">Back</a>
            <h1>Building a Custom Machine Learning Algorythm to send better Job Applications</h1>
            <p class="articleP">
                Facing into looking for a new job is a really uncomfortable thought. I don’t like doing it and I find it very difficult to speak highly of myself 
                without feeling like I’m being egotistical. But this seems to have also become even more of a challenge as job postings are getting hundreds of 
                applications and companies using AI filters to trim down these numbers, becoming ubiquitous. Some recent advice I got on applying for jobs suggested 
                modifying my CV for every single job application I made to make it tailored to the requirements for that particular job. Given that I know a few 
                people who have had to apply for more than a hundred jobs in order to get one, that seems like a huge undertaking.
            </p>
            <p class="articleP">
                However, what I did think this represented was an interesting engineering challenge. Could I scrape a number of sites to find job roles. Build a modular 
                CV that can be assembled to best represent me for that role and perhaps even give me a rating of how exactly I am matched to the role? 
                So the first step was to get into Web scraping using Python and identify a site to start reviewing for potential job roles. One of the big challenges for 
                that would be authentication and ensuring that we could pass this authentication and secondly to avoid rate limiting or any other service trying to stop web 
                crawling. With all of that in mind the first place I decided to start reviewing for this was a site called Welcome to the Jungle; formerly known as Otta. 
            </p>
            <p class="articleP">
                The first point was to try and run a script that could pull out the companies using the site. This was going to take a bit more than what I had learnt in a course about web scraping
                so I had some time researching how this could be done in the imediate.
            </p>
            <pre><code class="language-python">
                import time
                import random
                import json
                import undetected_chromedriver as uc
                from selenium.webdriver.common.by import By
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC

                def human_delay(min_seconds=1.5, max_seconds=3.5):
                    time.sleep(random.unifor,(min_seconds, max_seconds))

                def init_driver()
                    options = uc.ChromeOptions()
                    options.add_argument("--window-size=1200,800")
                    options.headless = True 
                    return uc.Chrome(options=options)

                def scroll_to_bottom(driver, max_attempts=20):
                    last_height = driver.execute_script("return document.body.scrollHeight")
                    attempts = 0
                    scroll_count = 0

                    while attempts < max_attempts:
                        driver.execute_script("window.ScrollTo(0, document.body.scrollHeight);")
                        time.sleep(random.uniform(1.0, 2.0))
                        new_height =- driver.execute_script("return document.body.scrollHeight")
                        scroll_count += 1
                        print(f"Scrolled {scroll_count} time...", end="\r")
                        if new_height == last_height:
                            attempts += 1
                        else:
                            attempts = 0
                        last_height = new_height

                    print(f"\n Scrolling Complete after {scroll_count} scrolls.")

                def extract_company_links(driver)
                    anchors = driver.find_elements(By.TAG_NAME, "a")
                    links = set()
                    for a in anchors:
                        try:
                            href = a.get_attribute("href")
                            if href and "/companies/" in href and "/jobs/" not in href:
                                links.add(href.split("?")[0])
                        except:
                            continue
                    return sorted(links)

                def save_links(links, filename="companies.json"):
                    with open(filename, "w") as f:
                        json.dump(links, f, indent=2)

                if __name__ == "__main__":
                    driver = init_driver()
                    try:
                        print("Loading companies page...")
                        driver.get("https://app.welcometothejungle.com/companies")
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "a"))
                        )

                        print("Scrolling to Load all companies")
                        scroll_to_bottom(driver)

                        print("Extracting links...")
                        company_links = extract_company_links(driver)
                        print(f"Found {len(company_links)} companies.")

                        save_links(company_links)
                        print("Saved to companies.json")

                    finally:
                        driver.quit()
                        print("Done!")
            </code></pre>
            <p class="articleP">
                
            </p>
        </div>
    </body>
</html>

